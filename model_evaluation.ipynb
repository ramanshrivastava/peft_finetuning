{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ramanshrivastava/ml_projects/peft_finetune/huggingface_env/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20164bbeb074b909df0a7dfebd995c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUNDATION model metrics {'eval_loss': 9.079292297363281, 'eval_accuracy': 0.0885, 'eval_runtime': 774.5768, 'eval_samples_per_second': 2.582, 'eval_steps_per_second': 0.041}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ramanshrivastava/ml_projects/peft_finetune/huggingface_env/lib/python3.9/site-packages/peft/tuners/lora/layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "trainable params: 78,336 || all params: 124,522,752 || trainable%: 0.06290898550009559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramanshrivastava/ml_projects/peft_finetune/huggingface_env/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb4277921044ee2a3c7e51cd791d34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1747, 'grad_norm': 17.788244247436523, 'learning_rate': 0.00175, 'epoch': 0.12}\n",
      "{'loss': 0.7962, 'grad_norm': 1.895721197128296, 'learning_rate': 0.0015, 'epoch': 0.25}\n",
      "{'loss': 0.7043, 'grad_norm': 3.2256157398223877, 'learning_rate': 0.00125, 'epoch': 0.38}\n",
      "{'loss': 0.6261, 'grad_norm': 14.52248477935791, 'learning_rate': 0.001, 'epoch': 0.5}\n",
      "{'loss': 0.6141, 'grad_norm': 4.336002349853516, 'learning_rate': 0.00075, 'epoch': 0.62}\n",
      "{'loss': 0.4759, 'grad_norm': 34.62288284301758, 'learning_rate': 0.0005, 'epoch': 0.75}\n",
      "{'loss': 0.4327, 'grad_norm': 13.375720024108887, 'learning_rate': 0.00025, 'epoch': 0.88}\n",
      "{'loss': 0.3659, 'grad_norm': 3.0427939891815186, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626187b6863640aeb702336b4c2ca085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3134929835796356, 'eval_accuracy': 0.9225, 'eval_runtime': 121.8424, 'eval_samples_per_second': 16.415, 'eval_steps_per_second': 4.104, 'epoch': 1.0}\n",
      "{'train_runtime': 2535.6444, 'train_samples_per_second': 6.31, 'train_steps_per_second': 1.578, 'train_loss': 0.6487208061218261, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b2681cc4844fc1a5e8d03af3b1df30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUNED model metrics {'eval_loss': 0.3134929835796356, 'eval_accuracy': 0.9225, 'eval_runtime': 122.4533, 'eval_samples_per_second': 16.333, 'eval_steps_per_second': 4.083, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion class: 0\n"
     ]
    }
   ],
   "source": [
    "from src.load_preprocess_dataset import load_tokenize_data\n",
    "from src.evaluation_metric import compute_metrics\n",
    "from src.foundation_model_evaluation import foundation_model_eval\n",
    "from src.finetune_lora import lora_model_tune_eval\n",
    "from src.finetuned_infer import predict_emotion\n",
    "\n",
    "def main():\n",
    "    #configuration\n",
    "    dataset_name = \"emotion\"\n",
    "    model_name = \"gpt2\"\n",
    "    num_labels = 6\n",
    "    text = \"im feeling quite sad and sorry for myself but ill snap out of it soon\"\n",
    "    model_checkpoint = \"./models/fine_tuned/checkpoint-4000\"\n",
    "\n",
    "    #tokenized data\n",
    "    tokenized_datset = load_tokenize_data(dataset_name, model_name)\n",
    "    print(tokenized_datset)\n",
    "    eval_dataset = tokenized_datset[\"validation\"]\n",
    "\n",
    "    #evaluate foundation model\n",
    "    results = foundation_model_eval(model_name,eval_dataset, num_labels, compute_metrics)\n",
    "    print(\"FOUNDATION model metrics\", results)\n",
    "\n",
    "    #fine-tune foundation model using lora and evaluate fine-tuned model\n",
    "    results_tuned = lora_model_tune_eval(model_name, tokenized_datset, num_labels, compute_metrics)\n",
    "    print(\"TUNED model metrics\", results_tuned)\n",
    "\n",
    "    #predict emotion\n",
    "    predicted_class = predict_emotion(text, model_checkpoint)\n",
    "    print(\"Predicted emotion class:\", predicted_class)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
